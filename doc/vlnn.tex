\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}
\usepackage{xspace}
\usepackage[margin=2.5cm]{geometry}
\usepackage{tikz}
\usepackage{pgfplots} 
\newcommand{\real}{\mathbb{R}}
\newcommand{\vv}{\operatorname{vec}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\vlnn}{\textsf{VLNN}\xspace}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bff}{\mathbf{f}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bfe}{\mathbf{e}}
\newcommand{\bone}{\mathbf{1}}

\tikzstyle{block} = [draw, rectangle, minimum height=3em, minimum width=3em] \tikzstyle{data} = []
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

% ------------------------------------------------------------------
\begin{document}
\title{CNNs for MATLAB}
\author{Andrea Vedaldi}
\maketitle{}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\section{Introduction}
% ------------------------------------------------------------------

\vlnn is a simple MATLAB toolbox implementing Convolutional Neural Networks (CNN) for computer vision applications. Its main features are:
\begin{itemize}
\item \emph{Flexibility.} Neural network layers are implemented in a straightforward manner, often directly in MATLAB code, so that they are easy to modify, extend, or integrate with new ones. Other toolboxes hide the whole neural network layers behind a wall of compiled code; here the granularity is much finer.
\item \emph{Power.} The implementation can run the latest features such as Krizhevsky~\textit{et al.}~\cite{krizhevsky12imagenet}, including the DeCAF and Caffe variants. Pre-learned features for different tasks can be easily downloaded.
\item \emph{Efficiency.} The implementation is quite efficient, supporting both CPU and GPU computation (in the latest versions of MALTAB). Despite MATLAB overhead, it is only marginally slower than alternative implementations.
\end{itemize}
This library will be merged in the future with VLFeat~(\url{http://www.vlfeat.org/}).

% ------------------------------------------------------------------
\subsection{Credits}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\section{}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\section{Evaluating CNNs and their derivatives}
% ------------------------------------------------------------------

CNNs are obtained by connecting one or more \emph{computational blocks}. Each block $\by = f(\bx,\bw)$ takes an image or map $\bx$ and a set of parameters $\bw$ as input and produces a new image or map $\by$ as output. An image/map is a real 4D array; the first two dimensions index spatial coordinates (image rows and columns respectively), the third dimension feature channels, and the last dimension instances. A block is therefore represented as follows:
\begin{center}
\begin{tikzpicture}[auto, node distance=2cm]
\node (x) [data] {$\bx$};
\node (f) [block,right of=x]{$f$};
\node (y) [data, right of=f] {$\by$};
\node (w) [data, below of=f] {$\bw$};
\draw [->] (x.east) -- (f.west) {};
\draw [->] (f.east) -- (y.west) {};
\draw [->] (w.north) -- (f.south) {};
\end{tikzpicture}
\end{center}
Formally, $\bx$ is a 4D tensor stacking $N$ 3D maps
\[
   \bx \in \real^{H \times W \times D \times N}
\]
where $H$ and $W$ are the height and width of the maps, $D$ its depth, and $N$ the number of maps. In what follows, all operations are applied identically to each map in the stack $\bx$; hence for simplicity we will drop the last dimension in our discussion, but this feature is important to operate efficiently on batches of data.

We are often interested in computing the derivative of the block with respect to the data $\bx$ and the parameters $\bw$. In fact, we usually look at the block $f$ as a component of a scalar-valued function $z \circ f(\bx,\bw) \in \real$ (the latter usually combines the model and the learning loss):
\begin{center}
\begin{tikzpicture}[auto, node distance=2cm]
\node (x) [data] {$\bx$};
\node (f) [block,right of=x ] {$f$};
\node (bz)[block,right of=f ] {$z(\cdot)$};
\node (z) [data, right of=bz] {$z$};
\node (w) [data, below of=f ] {$\bw$};
\draw [->] (x.east) -- (f.west) {};
\draw [->] (f.east) -- node {$\by$}  (bz.west) {};
\draw [->] (w.north) -- (f.south) {};
\draw [->] (bz.east) -- (z.west) {};
\end{tikzpicture}
\end{center}
In this case, the derivative of $z$ with respect $\bx$ and $\bw$ is given by:
\[
\frac{dz}{d(\vv \bx)^\top}
=
\frac{dz}{d(\vv \by)^\top}
\frac{d\vv f}{d(\vv \bx)^\top},
\quad
\frac{dz}{d(\vv \bw)^\top}
=
\frac{dz}{d(\vv \by)^\top}
\frac{d\vv f}{d(\vv \bw)^\top},
\]
The $\vv$ symbol is the vectorization operator, which simply reshape its argument to a column vector. This notation for the derivatives is taken from~\cite{kinghorn96integrals} and is fairly useful in calculations.

Note that, in order to compute the derivatives of $z \circ f$ with respect to the data and parameters, one needs to provide the derivative of $z$ with respect to the block output $\by$ (backprop). Furthermore, the $\vv$ notation is somewhat an overkill for the programming interface; in fact, one always computes derivatives of the scalar $z$ and these can be represented as arrays $\frac{dz}{d\bx}, \frac{dz}{d\by}, \frac{dz}{d\bw}$ of the same size of $\bx,\by,\bw$.


\paragraph{Interconnections.} In general, a CNN can be obtained by connecting blocks in a directed acyclic graph (DAG). In the simplest case, this graph reduces to a sequence of computational blocks. Let $\bx_1,\bx_2,\dots,\bx_L$ be the output of each layer in the network, where $\bx_1$ is a pseudo-output used to store the input data processed by the network. Each output $\bx_l$ depends on the previous output $\bx_{l-1}$ through a function $f_l$ and parameter $w_l$ as $\bx_l = f_l(\bx_{l-1},\bw_l)$; schematically:
\begin{center}
\begin{tikzpicture}[auto, node distance=2cm]
\node (x1)  [data] {$x_1$};
\node (f2) [block,right of=x1]{$f_2$};
\node (f3) [block,right of=f2,node distance=3cm]{$f_3$};
\node (dots) [right of=f3]{...};
\node (fL) [block,right of=dots]{$f_L$};
\node (xL)  [data, right of=fL] {$x_L$};
\node (w2) [data, below of=f2] {$w_2$};
\node (w3) [data, below of=f3] {$w_3$};
\node (wL) [data, below of=fL] {$w_L$};
\draw [->] (x1.east) -- (f2.west) {};
\draw [->] (f2.east) -- node {$x_2$} (f3.west);
\draw [->] (f3.east) -- node {$x_3$} (dots.west) {};
\draw [->] (dots.east) -- node {$x_{L-1}$} (fL.west) {};
\draw [->] (fL.east) -- (xL.west) {};
\draw [->] (w2.north) -- (f2.south) {};
\draw [->] (w3.north) -- (f3.south) {};
\draw [->] (wL.north) -- (fL.south) {};
\end{tikzpicture}
\end{center}
Given an input $x_1$, evaluating the network is a simple matter of evaluating all the intermediate stages in order to compute an overall function $x_L = f(x_1;w_2,\dots,w_L)$. The derivatives can be computed in a similar manner, but starting from the end of the DAG. For example:
\[
 \frac{dz}{d(\vv\bw_l)^\top}
 =
 \frac{dz}{d(\vv\bx_L)^\top}
 \frac{d\vv\bx_{L}}{d(\vv\bx_{L-1})^\top}
 \dots
 \frac{d\vv\bx_{l+1}}{d(\vv\bx_{l})^\top}
 \frac{d\vv\bx_{l}}{d(\vv\bw_{l})^\top}
\]

% ------------------------------------------------------------------
\section{Computational blocks}
% ------------------------------------------------------------------

This section describes the individual computational block supported by the package. In MATLAB a neural network block is implemented as a map \verb!y = vl_nn<block>(x,w)! that takes as input arrays \verb!x! and \verb!w! representing the input and parameters of the layer and returns an array \verb!y! as output. \verb!x! and \verb!y! are 4D real arrays packing $N$ maps or images, as discussed above. 

In order to compute derivatives, the function can take a third optional argument \verb!dzdy! representing the derivative of the output of the network with respect to $y$ and returns the corresponding derivatives \verb![dzdx,dzdw] = vl_nn<block>(x,w,dzdy)!. \verb!dzdx!, \verb!dzdy! and \verb!dzdw! are array with the same dimension of \verb!x!, \verb!y! and \verb!w! respectively.

A function can take additional optional arguments, specified as a property-value list; it can take no parameters (e.g. a rectified linear unit), in which case \verb!w! is empty; it can take multiple inputs and parameters, in which case \verb!x!, \verb!w!, \verb!dzdx!, \verb!dzdw! are cell arrays containing a corresponding number of arrays.

% ------------------------------------------------------------------
\subsection{Convolution}\label{s:convolution}
% ------------------------------------------------------------------

The convolutional block is implemented by the function \verb!vl_nnconv!. \verb!y=vl_nnconv(x,f,b)! computes the convolution of the input map $\bx$ with a bank of $K$ multi-dimensional filters $\bff$ and biases $b$. Here
\[
 \bx\in\real^{H \times W \times D}, \quad
 \bff\in\real^{H' \times W' \times D \times K}, \quad
 \by\in\real^{H'' \times W'' \times K}, \quad
 \quad
 W'' = W - W' + 1,
 \quad
 H'' = H - H' + 1,
\]
Formally, the output  is given by
\[
y_{i''j''k}
=
b_k
+
\sum_{i'=1}^{H'}
\sum_{j'=1}^{W'}
\sum_{d=1}^D
f_{i'j'd} \times x_{i''+i',j''+j',d,k}.
\]
The call \verb!vl_nnconv(x,f,[])! does not use the biases.

\paragraph{Output size, padding, and sampling stride.} The convolution operator can be adapted to account for image padding and subsampling. Suppose that the input image or map $\bx$ has width $W$ and that the filter $\bff$ has width $W' \leq W$. Then there are 
\[
  W'' = W - W' + 1
\]
possible translations of the filters in the horizontal direction such that the filter is entirely contained in the input $\bx$. Hence, by default the filtered signal $\by$ has width $W''$. However, \verb!vl_nnconv! accepts a padding parameter $P$ whose effect is to virtually pad with zeros the signal $\bx$ in all spatial directions. In this case, the output signal has width
\[
  W'' = W + 2P - W' + 1.
\]
\verb!vl_nnconv! also accepts a stride parameter $\delta$ to subsample the output. In this case, if $j$ is the column index of the output signal $\by$, its maximum value is given by:
\[
(j-1)\delta + W' \leq W + 2P.
\]
Hence the width of $\by$ is given by
\[
W'' = \lfloor
\frac{W + 2P - W'}{\delta}
\rfloor + 1
\]
samples. Similar relations apply to the signal heights $H,H'$ and $H''$.

\paragraph{Fully connected layer.} This library does not distinguishes between fully connected layers and convolutional blocks. Instead, the former is a special case of the latter obtained when the output map $\by$ has dimensions $W''=H''=1$. Internally, \verb!vl_nnconv! handle this case more efficiently if possible.

\paragraph{Filter groups.} For additional flexibility, \verb!vl_nnconv! allows to group input feature channels and apply to them different filter groups. To to do so, specify as input a bank  of $K$ filters $\bff\in\real^{H'\times W'\times D'\times K}$ such that $D'$ divides the number of input dimensions $D$. These are treated as $g=D/D'$ filter groups; the first group is applied to dimensions $d=1,\dots,D'$ of the input $\bx$; the second group to dimensions $d=D'+1,\dots,2D'$ and so on. Note that the ouptut is still an array $\by\in\real^{H''\times W''\times K}$.

An application of grouping is implementing the Krizhevsky and Hinton network~\cite{krizhevsky12imagenet}, which uses two such streams. Another application is sum pooling; in the latter case, one can specify $D$ groups of $D'=1$ dimensional filters.

\paragraph{Matrix notation.} It is often convenient to express the convolution operation in matrix form. To this end, let $\phi(\bx)$ the {\tt im2row} operator, extracting all $W' \times H'$ patches from the map $\bx$ and storing them as rows of a $(H''W'') \times (H'W'D)$ matrix:
\[
   [\phi(\bx)]_{pq} \underset{(i,j,d)=t(p,q)}{=} x_{ijd}
\]
according to the mapping $(i,j,d) = t(p,q)$ given by
\[
 i = i''+i'-1, \quad
 j = j''+j'-1, \quad
 p = i'' + H'' (j''-1), \quad
 q = i' + H'(j'-1) + H'W' (d-1).
\]
It is also useful to define the ``transposed'' operator {\tt row2im}:
\[
   [\phi^*(M)]_{ijd}
   =
   \sum_{(p,q) \in t^{-1}(i,j,d)}
   M_{pq}.
\]
Note that $\phi$ and $\phi^*$ are linear operators. Hence we can express both operators denoting by $H\in\real^{(H''W''H'W'D) \times(HWD)}$ the matrix such that
\[
  \vv(\phi(\bx)) = H \vv(\bx), \qquad 
  \vv(\phi^*(M)) = H^\top \vv(M)
\]
where $\vv()$ is the vectorization operator. Hence:
\[
 \vv\by = 
 \vv\left(\phi(\bx) F\right)
 =
 \begin{cases}
 (I \otimes \phi(\bx)) \vv F, & \text{or, equivalently,} \\
 (F^\top \otimes I) \vv \phi(\bx),
 \end{cases}
\]
where $F\in\mathbb{R}^{(H'W'D)\times K}$ is the matrix obtained by reshaping the array $\bff$ and $I$ is an identity matrix of suitable dimensions. Hence
\[
\frac{dz}{d(\vv F)^\top}
=
\frac{dz}{d(\vv\by)^\top}
(I \otimes \phi(\bx))
= \vv\left[ 
\phi(\bx)^\top 
\frac{dz}{dY}
\right]^\top
\]
where $Y\in\real^{(H''W'')\times K}$ is the matrix obtained by reshaping the array $\by$. Likewise:
\[
\frac{dz}{d(\vv \bx)^\top}
=
\frac{dz}{d(\vv\by)^\top}
(F^\top \otimes I)
\frac{d\vv \phi(\bx)}{d(\vv \bx)^\top}
=
\vv\left[ 
\frac{dz}{dY}
F^\top
\right]^\top
H
\]
Hence, the derivatives are given by
\[
\boxed{
\frac{dz}{dF}
=
\phi(\bx)^\top\frac{d z}{d Y},
\qquad
\frac{d z}{d X}
=
\phi^*\left(
\frac{d z}{d Y}F^\top
\right)
}
\]
where $X\in\real^{(H'W')\times D}$ is the matrix obtained by reshaping $\bx$.

% ------------------------------------------------------------------
\subsection{Max pooling}\label{s:pooling}
% ------------------------------------------------------------------

\verb!vl_nnpool! implements max pooling. This operator computes the maximum response of each feature channel in a $H' \times W'$ patch
\[
y_{i''j''d} = \max_{1\leq i' \leq H', 1 \leq j' \leq W'} x_{i''+i',j''+j',d}.
\]
resulting in an output of size $\by\in\real^{H''\times W'' \times D}$, similar to the convolution operator of Sect.~\ref{s:convolution}.

\paragraph{Padding and stride.} Similar to the convolution operator of Sect.~\ref{s:convolution}, \verb!vl_nnpool! supports padding the input by $P$ samples, but in this case with the value $-\infty$. It also supports subsampling the output with a stride of $\delta$ elements.

\paragraph{Matrix notation.} Since max pooling simply select for each output element an input element, the relation can be expressed in matrix form as
$
    \vv\by = S \vv \bx
$
for a suitable selector matrix $S\in\{0,1\}^{(H''W''D) \times (HWD)}$. The derivatives can the be written as:
$
\frac{d z}{d (\vv \bx)^\top}
=
\frac{d z}{d (\vv \by)^\top}
S,
$
or in summary:
\begin{equation}\label{e:max-mat}
\boxed{
\vv\by = S \vv \bx,
\qquad
\frac{d z}{d \vv \bx}
=
S^\top
\frac{d z}{d \vv \by}.
}
\end{equation}

% ------------------------------------------------------------------
\subsection{ReLU}\label{s:relu}
% ------------------------------------------------------------------

\verb!vl_nnrelu! computes the \emph{Rectified Linear Unit} (ReLU):
\[
 y_{ijd} = \max\{0, x_{ijd}\}.
\]

\paragraph{Matrix notation.} With matrix notation, we can express the ReLU as
\[
\boxed{
\vv\by = \diag\bs \vv \bx,
\qquad
\frac{d z}{d \vv \bx}
=
\diag\bs
\frac{d z}{d \vv \by}
}
\]
where $\bs = [\vv \bx > 0] \in\{0,1\}^{HWD}$ is an indicator vector.

% ------------------------------------------------------------------
\subsection{Normalization}\label{s:normalization}
% ------------------------------------------------------------------

\verb!vl_nnnormalize! implements a cross-channel normalization operator. Normalization applied independently at each spatial location and groups of channels to get:
\[
 y_{ijk} = x_{ijk} \left( \kappa + \alpha \sum_{t\in G(k)} x_{ijt}^2 \right)^{-\beta},
\]
where, for each output channel $k$, $G(k) \subset \{1, 2, \dots, D\}$ is a corresponding subset of input channels. Note that input $\bx$ and output $\by$ have the same dimensions. Note also that the operator is applied across feature channels in a convolutional manner.


\paragraph{Implementation details.} The derivative is easily computed as:
\[
\frac{dz}{d x_{ijd}}
=
\frac{dz}{d y_{ijd}}
L(i,j,d|\bx)^{-\beta}
-2\alpha\beta
\sum_{k:d\in G(k)}
\frac{dz}{d y_{ijk}}
L(i,j,k|\bx)^{-\beta-1} x_{ijk} x_{ijd}
\]
where
\[
 L(i,j,k|\bx) = \kappa + \alpha \sum_{t\in G(k)} x_{ijt}^2.
\]

% ------------------------------------------------------------------
\subsection{Softmax}\label{s:softmax}
% ------------------------------------------------------------------

\verb!vl_softmax! computes the softmax operator:
\[
 y_{ijk} = \frac{e^{x_{ijk}}}{\sum_{t=1}^D e^{x_{ijt}}}.
\]
Note that the operator is applied across feature channels in a convolutional manner.

\paragraph{Implementation details.} Care must be taken in evaluating the exponential in order to avoid underflow or overflow. The simplest way to do so is to divide from numerator and denominator by the maximum value:
\[
 y_{ijk} = \frac{e^{x_{ijk} - \max_d x_{ijd}}}{\sum_{t=1}^D e^{x_{ijt}- \max_d x_{ijd}}}.
\]
The derivative is given by:
\[
\frac{dz}{d x_{ijd}}
=
\sum_{k}
\frac{dz}{d y_{ijk}}
\left(
e^{x_{ijd}} L(\bx)^{-1} \delta_{\{k=d\}}
-
e^{x_{ijd}}
e^{x_{ijk}} L(\bx)^{-2}
\right),
\quad
L(\bx) = \sum_{t=1}^D e^{x_{ijt}}.
\]
Simplifying:
\[
\frac{dz}{d x_{ijd}}
=
y_{ijd} 
\left(
\frac{dz}{d y_{ijd}}
-
\sum_{k=1}^K
\frac{dz}{d y_{ijk}} y_{ijk}.
\right).
\]
In matrix for:
\[
  \frac{dz}{dX} = Y \odot \left(\frac{dz}{dY} 
  - \left(\frac{dz}{dY} \odot Y\right) \bone\bone^\top\right)
\]
where $X,Y\in\real^{HW\times D}$ are the matrices obtained by reshaping the arrays
$\bx$ and $\by$. Note that this expression is numerically uncomplicated once the output $Y$ has been computed.

% ------------------------------------------------------------------
\subsection{Log-loss}\label{s:loss}
% ------------------------------------------------------------------

\verb!vl_logloss! computes the \emph{logarithmic loss}
\[
 y = \ell(\bx,c) = - \sum_{ij} \log x_{ijc}
\]
where $c \in \{1,2,\dots,D\}$ is the g.t\@. class.

\paragraph{Implementation details.} The derivative is
\[
\frac{dz}{dx_{ijd}} = - \frac{dz}{dy} \frac{1}{x_{ijc}} \delta_{\{d = c\}}.
\]
Note that one takes the average loss on all the training data.


% ------------------------------------------------------------------
\subsection{Softmax log-loss}\label{s:sfloss}
% ------------------------------------------------------------------

\verb!vl_softmaxloss! combines the softmax layer and the log-loss into one step for improved numerical stability. It computes
\[
y = - \sum_{ij} \left(
x_{ijc} - \log \sum_{d=1}^D e^{x_{ijd}}
\right)
\]
where $c$ is the g.t. class.

\paragraph{Implementation details.} The derivative is given by
\[
\frac{dz}{dx_{ijd}} 
= - \frac{dz}{dy} \left(\delta_{d=c} - y_{ijc}\right)
\]
where $y_{ijc}$ is the output of the softmax layer. In matrix form:
\[
\frac{dz}{dX} 
= - \frac{dz}{dy} \left(\bone^\top \bfe_c - Y\right)
\]
where $X,Y\in\real^{HW\times D}$ are the matrices obtained by reshaping the arrays
$\bx$ and $\by$ and $\bfe_c$ is the indicator vector of class $c$.


% ------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{references}
% ------------------------------------------------------------------

% ------------------------------------------------------------------
\appendix\section{Proofs}\label{s:proofs}
% ------------------------------------------------------------------

\end{document}


